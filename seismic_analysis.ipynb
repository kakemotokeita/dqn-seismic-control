{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6-final"
    },
    "colab": {
      "name": "seismic_analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kakemotokeita/dqn-seismic-control/blob/main/seismic_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA6U-6UEEu_1"
      },
      "source": [
        "# 深層強化学習による振動制御に挑戦してみた\n",
        "\n",
        "この記事は、[AEC and Related Tech Advent Calendar 2020](https://adventar.org/calendars/5473) の16日目の記事です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vs8bRccgCsD"
      },
      "source": [
        "## はじめに\n",
        "巷で話題のAIといういうものを学ぶべく、深層強化学習DQN（Deep Q Network）を用いて1質点系の振動を制御するプログラムの作成に挑戦しました。深層強化学習は、深層学習と強化学習を組み合わせることで、ルールを教えなくとも報酬を最大化するように学習し、ニューラルネットワークを構成します。深層強化学習には、オープンソースの機械学習ライブラリであるPytorchを用います。また、時刻歴応答解析には、オープンソースの構造解析ライブラリであるOpenSeesのPythonインタープリターであるOpenSeesPyを用います。\n",
        "\n",
        "作成にあたっては、以下の３つのウェブサイトを大いに参考にさせていただきました。機械学習の概要や深層強化学習、OpenSeesの使い方については、私が説明するよりも以下のサイトの方がずっと丁寧にわかりやすく解説があるので、こちらを見るのがおすすめです。\n",
        "\n",
        "* [メディカルAI専門コース オンライン講義資料](https://japan-medical-ai.github.io/medical-ai-course-materials/)\n",
        "\n",
        "機械学習の基礎を学ぶ用。基礎から学ぶのに超おすすめ。医療系をテーマにしていますが、すごく丁寧にわかりやすく説明されているので医療系でない私でも全く問題ありませんでした。プログラムもChainerがベースですが、Pytorchに通じる部分が多いので問題ありません。\n",
        "\n",
        "* [REINFORCEMENT LEARNING (DQN) TUTORIAL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
        "\n",
        "Pytorchを用いた深層強化学習の具体的な実装方法の公式チュートリアル。今回のプログラムの強化学習は主にこちらを元にして書いています。プログラムのより詳しい説明はこちらを参考にしていただくわかりやすいと思います。\n",
        "\n",
        "* [The OpenSeesPy Library](https://openseespydoc.readthedocs.io/en/latest/)\n",
        "\n",
        "OpenSeesPyの公式ドキュメント。やっぱり公式！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NyvMnM-gCsD"
      },
      "source": [
        "## 時刻歴応答解析概要\n",
        "* モデル: 免震建物をイメージして固有周期4秒の1質点系。剛性は弾性。減衰は剛性比例とし、0〜1で変化させる。\n",
        "* 入力地震動: サンプル波 https://github.com/kakemotokeita/dqn-seismic-control/blob/master/wave/sample.csv サンプリング周波数50hz\n",
        "* 解析方法: 直接積分法\n",
        "* 積分方法: 平均加速度法\n",
        "* 時間刻み: 0.02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OPQI2F4gCsE"
      },
      "source": [
        "## 振動制御の概要と目標\n",
        "\n",
        "今回は、対象モデルにおいて、どのように減衰定数を変化させれば、サンプル地震動入力時の絶対応答加速度を最小化できるのかを学習させることに挑戦してみます。減衰定数は、時間刻みごとに変化させます。\n",
        "\n",
        "### 参考結果\n",
        "一概には言えませんが、通常免震建物は減衰定数を20%程度とすることで、相対応答変位・絶対応答加速度をバランスよく低減できることが多いです。参考値として減衰定数を20%で一定とした時の解析結果を以下に示します。応答のばらつきの指標として、標準偏差も示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxXxVctegCsE"
      },
      "source": [
        "h_ave= 0.200   # 減衰定数平均値[-]\n",
        "h_sd= 0.000    # 減衰定数標準偏差[-]\n",
        "acc_sd= 0.233  # 絶対応答加速度標準偏差[m/s2]\n",
        "dis_sd= 0.024  # 相対応答変位標準偏差[m]\n",
        "acc_max= 3.863 # 最大絶対応答加速度[m/s2]\n",
        "dis_max= 0.125 # 最大相対応答変位[m]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyCV7pTVQu1Z"
      },
      "source": [
        "## 解析クラス\n",
        "\n",
        "解析用の`Analysis`クラスを設定します。このクラスでは`step`メソッドが呼ばれると次のステップが計算され、報酬（`reward`）と解析完了かどうか(`done`)を返すようにします。今回の目的は、絶対応答加速度を最小化することです。そのため、ここでは報酬を、ステップ計算後の絶対加速度に反比例する値としています。\n",
        "\n",
        "振動解析エンジンには、前述の通りOpenSeesPyを用います。\n",
        "\n",
        "ネットワークが選択できるアクションとしては、0〜1の減衰定数を選べるようにしており、 `step`メソッドでアクションの番号を引数として受け取れるようになっています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJUFsUVUQ1Ia"
      },
      "source": [
        "!pip install openseespy\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import openseespy.opensees as op\n",
        "\n",
        "FREE = 0\n",
        "FIXED = 1\n",
        "\n",
        "X = 1\n",
        "Y = 2\n",
        "ROTZ = 3\n",
        "\n",
        "\n",
        "class Analysis:\n",
        "\n",
        "    def __init__(self):\n",
        "        # ネットワークが取れるアクションの設定\n",
        "        self.action = np.array([0, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
        "        self.naction = len(self.action)\n",
        "\n",
        "        self.beta = 1/4\n",
        "\n",
        "        # 1質点系モデル\n",
        "        self.T0 = 4\n",
        "        self.h = self.action[0]\n",
        "        self.hs = [self.h]\n",
        "        self.m = 100\n",
        "        self.k = 4*np.pi**2*self.m/self.T0**2\n",
        "\n",
        "        # 入力地震動\n",
        "        self.dt = 0.02\n",
        "        to_meter = 0.01  # cmをmに変換する値\n",
        "        self.wave_url = 'https://raw.githubusercontent.com/kakemotokeita/dqn-seismic-control/main/wave/sample.csv'\n",
        "        with urllib.request.urlopen(self.wave_url) as wave_file:\n",
        "            self.wave_data = np.loadtxt(wave_file, usecols=(0,), delimiter=',', skiprows=3)*to_meter\n",
        "\n",
        "        # OpenSees設定\n",
        "        op.wipe()\n",
        "        op.model('basic', '-ndm', 2, '-ndf', 3)  # 2 dimensions, 3 dof per node\n",
        "\n",
        "        # 節点\n",
        "        self.bot_node = 1\n",
        "        self.top_node = 2\n",
        "        op.node(self.bot_node, 0., 0.)\n",
        "        op.node(self.top_node, 0., 0.)\n",
        "\n",
        "        # 境界条件\n",
        "        op.fix(self.top_node, FREE, FIXED, FIXED)\n",
        "        op.fix(self.bot_node, FIXED, FIXED, FIXED)\n",
        "        op.equalDOF(1, 2, *[Y, ROTZ])\n",
        "\n",
        "        # 質量\n",
        "        op.mass(self.top_node, self.m, 0., 0.)\n",
        "\n",
        "        # 弾性剛性\n",
        "        elastic_mat_tag = 1\n",
        "        Fy = 1e10\n",
        "        E0 = self.k\n",
        "        b = 1.0\n",
        "        op.uniaxialMaterial('Steel01', elastic_mat_tag, Fy, E0, b)\n",
        "\n",
        "        # エレメントの設定\n",
        "        beam_tag = 1\n",
        "        op.element('zeroLength', beam_tag, self.bot_node, self.top_node, \"-mat\", elastic_mat_tag, \"-dir\", 1, '-doRayleigh', 1)\n",
        "\n",
        "        # 外力の設定\n",
        "        load_tag_dynamic = 1\n",
        "        pattern_tag_dynamic = 1\n",
        "\n",
        "        self.values = list(-1 * self.wave_data)  # should be negative\n",
        "        op.timeSeries('Path', load_tag_dynamic, '-dt', self.dt, '-values', *self.values)\n",
        "        op.pattern('UniformExcitation', pattern_tag_dynamic, X, '-accel', load_tag_dynamic)\n",
        "\n",
        "        # 減衰の設定\n",
        "        self.w0 = op.eigen('-fullGenLapack', 1)[0] ** 0.5\n",
        "        self.alpha_m = 0.0\n",
        "        self.beta_k = 2 * self.h / self.w0\n",
        "        self.beta_k_init = 0.0\n",
        "        self.beta_k_comm = 0.0\n",
        "\n",
        "        op.rayleigh(self.alpha_m, self.beta_k, self.beta_k_init, self.beta_k_comm)\n",
        "\n",
        "        # 解析の設定\n",
        "\n",
        "        op.wipeAnalysis()\n",
        "\n",
        "        op.algorithm('Newton')\n",
        "        op.system('SparseGeneral')\n",
        "        op.numberer('RCM')\n",
        "        op.constraints('Transformation')\n",
        "        op.integrator('Newmark', 0.5, 0.25)\n",
        "        op.analysis('Transient')\n",
        "\n",
        "        tol = 1.0e-10\n",
        "        iterations = 10\n",
        "        op.test('EnergyIncr', tol, iterations, 0, 2)\n",
        "        self.i_pre = 0\n",
        "        self.i = 0\n",
        "        self.i_next = 0\n",
        "        self.time = 0\n",
        "        self.analysis_time = (len(self.values) - 1) * self.dt\n",
        "        self.dis = 0\n",
        "        self.vel = 0\n",
        "        self.acc = 0\n",
        "        self.a_acc = 0\n",
        "        self.force = 0\n",
        "        self.resp = {\n",
        "            \"time\": [],\n",
        "            \"dis\": [],\n",
        "            \"acc\": [],\n",
        "            \"a_acc\": [],\n",
        "            \"vel\": [],\n",
        "            \"force\": [],\n",
        "        }\n",
        "        self.done = False\n",
        "\n",
        "    # 初期化\n",
        "    def reset(self):\n",
        "        self.__init__()\n",
        "\n",
        "    # １ステップ分の計算を行う\n",
        "    def step(self, action=0):\n",
        "        self.time = op.getTime()\n",
        "        assert(self.time < self.analysis_time)\n",
        "\n",
        "        # 選ばれたアクションに応じて減衰定数を変化させる\n",
        "        self.h = self.action[action]\n",
        "        self.hs.append(self.h)\n",
        "        self.beta_k = 2 * self.h / self.w0\n",
        "        op.rayleigh(self.alpha_m, self.beta_k, self.beta_k_init, self.beta_k_comm)\n",
        "\n",
        "        op.analyze(1, self.dt)\n",
        "        op.reactions()\n",
        "\n",
        "        self.dis = op.nodeDisp(self.top_node, 1)\n",
        "        self.vel = op.nodeVel(self.top_node, 1)\n",
        "        self.acc = op.nodeAccel(self.top_node, 1)\n",
        "        self.a_acc = self.acc + self.values[self.i]\n",
        "        self.force = -op.nodeReaction(self.bot_node, 1) # Negative since diff node\n",
        "\n",
        "        self.resp[\"time\"].append(self.time)\n",
        "        self.resp[\"dis\"].append(self.dis)\n",
        "        self.resp[\"vel\"].append(self.vel)\n",
        "        self.resp[\"acc\"].append(self.acc)\n",
        "        self.resp[\"a_acc\"].append(self.a_acc)\n",
        "        self.resp[\"force\"].append(self.force)\n",
        "\n",
        "        next_time = op.getTime()\n",
        "        self.done = next_time >= self.analysis_time\n",
        "\n",
        "        self.i_pre = self.i\n",
        "        self.i += 1\n",
        "        self.i_next = self.i + 1 if not self.done else self.i\n",
        "        return self.reward, self.done\n",
        "\n",
        "    # 報酬\n",
        "    @property\n",
        "    def reward(self):\n",
        "        return (10 / np.abs(self.a_acc))**3\n",
        "\n",
        "    # 選ばれた減衰の平均値(参考値)\n",
        "    @property\n",
        "    def h_ave(self):\n",
        "        return np.average(self.hs)\n",
        "\n",
        "    # 選ばれた減衰の分散(参考値)\n",
        "    @property\n",
        "    def h_sd(self):\n",
        "        return np.sqrt(np.var(self.hs))\n",
        "\n",
        "    # 振動解析の現在の状態\n",
        "    @property\n",
        "    def state(self):\n",
        "        return np.array([self.values[self.i_pre], self.values[self.i], self.values[self.i_next], self.a_acc, self.acc, self.vel, self.dis], dtype=np.float32)\n",
        "\n",
        "    # 平均値\n",
        "    @property\n",
        "    def sd(self):\n",
        "        return np.sqrt(np.var(np.abs(self.resp[\"a_acc\"]))), np.sqrt(np.var(np.abs(self.resp[\"dis\"])))\n",
        "\n",
        "    # 最大値\n",
        "    @property\n",
        "    def max(self):\n",
        "        return np.max(np.abs(self.resp[\"a_acc\"])), np.max(np.abs(self.resp[\"dis\"]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XdiV6syso0Q"
      },
      "source": [
        "## 経験の記録\n",
        "\n",
        "経験を記録しておくための`ReplayMemory`クラスを作成します。記録するのは、`state`(現在の状態), `action`(実行したアクション), `next_state`(アクション後の状態), `reward`(報酬)です。記録した中からランダムにデータをサンプリングするメソッドも追加しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syGu4sLks7vJ"
      },
      "source": [
        "import random\n",
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        # Transitionを記録する\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # ランダムなサンプルをリターン\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFhmo1CGul3x"
      },
      "source": [
        "## 深層学習\n",
        "DQNのモデルを作成します。今回は、３つの畳み込み層と１つの全結合層で構成しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAG3zDMrusMh"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, s, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 16, kernel_size=2, stride=1)\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=2, stride=1)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.conv3 = nn.Conv1d(32, 32, kernel_size=2, stride=1)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "\n",
        "        # conv1dから出力されるサイズの計算\n",
        "        def conv1d_out_size(size, kernel_size=2, stride=1):\n",
        "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "        # conv1d3回分の出力サイズを計算\n",
        "        conv = conv1d_out_size(conv1d_out_size(conv1d_out_size(s)))\n",
        "        linear_input_size = conv * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # ネットワークの順伝播を計算して計算結果を返す\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgMJobVOvkvF"
      },
      "source": [
        "## 作成したモデルの準備\n",
        "これまで作成したクラスを用いて、強化学習の準備を行います。Analysisを強化学習の環境として設定し、そのstateを呼び出せるように関数を作成しておきます。最適化手法は今回、[`RMSprop`](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop)を用いることとします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKyS4fu3wj65"
      },
      "source": [
        "import math\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# 振動解析を環境としてインスタンス化\n",
        "env = Analysis()\n",
        "\n",
        "# GPUが使えるかどうか\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"use {device}\")\n",
        "\n",
        "\n",
        "def get_state():\n",
        "    return Variable(torch.from_numpy(env.state)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "# DQNをインスタンス化するためのサイズ取得\n",
        "init_state = get_state()\n",
        "_, _, state_size = init_state.size()\n",
        "\n",
        "n_actions = env.naction     # 選択できるアクションの数\n",
        "\n",
        "policy_net = DQN(state_size, n_actions).to(device)  # 方策を求めるためのネットワーク\n",
        "target_net = DQN(state_size, n_actions).to(device)  # 最適化対象のネットワーク\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval() # 推論モードにする\n",
        "\n",
        "# 最適化アルゴリズムにはRMSpropを選択\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(10000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-_Y8Q1WzDGL"
      },
      "source": [
        "## アクションの選択\n",
        "あるstateを与えられた時、どのアクションを選択するかを判断する関数を設定します。１エピソードのうち始めの方ではランダムなアクションが選択されやすく、後になるにつれてニューラルネットワークから判断したアクションをとるような設定としています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rq_OLvmzHHB"
      },
      "source": [
        "BATCH_SIZE = 128    # 複数の結果をまとめてニューラルネットワークに入力、分析する際のバッチサイズ\n",
        "GAMMA = 0.5       # 遠い側の未来を考慮する割合（0に近いほど近い未来に重きをおく）\n",
        "\n",
        "# ランダムのアクションを選択する閾値計算用の係数\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 800\n",
        "\n",
        "TARGET_UPDATE = 100  # target_netを更新するエピソードの間隔\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "# あるstateでアクションを選択する関数\n",
        "def select_action(state, test):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if test:\n",
        "        with torch.no_grad():\n",
        "            return target_net(state).max(1)[1].view(1, 1)\n",
        "    elif sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # 最も効果的と思われるアクションのインデックス\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        # ランダムなアクションのインデックス\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucJT3uUj1LpK"
      },
      "source": [
        "## モデルの最適化\n",
        "最適化では、実際に取ったアクションの価値と、本来期待されたアクションの価値から損失を計算し、損失を小さくするようにモデルを更新していきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36FyRYT01XgQ"
      },
      "source": [
        "# モデルの最適化\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    # memoryからBATCH_SIZE分だけランダムにサンプルを取得\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # 解析終了時のステップ以外かどうかのBooleanとその時のnext_state\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # 実際に取ったアクションの価値（実際に取って得られた報酬）\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # まだ更新されていないTarget_netによる最も大きい報酬\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "    # 本来期待されたアクションの価値\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Huber loss（実際に取ったアクションの価値と、本来期待されたアクションの価値を比較して損失を計算）\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1).type(torch.FloatTensor))\n",
        "\n",
        "    # モデルの最適化\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgpCV-_v2wSU"
      },
      "source": [
        "## 学習の実行\n",
        "指定したエピソードの回数だけ学習を行います。今回は`target_net`を更新するタイミングで、学習の成果を確認できるように設定しました。学習の効果を確認するために、応答の標準偏差と最大値を出力しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnJSNRUc2zoF"
      },
      "source": [
        "num_episodes = 500\n",
        "for i_episode in range(num_episodes + 1):\n",
        "    # 環境の初期化\n",
        "    env.reset()\n",
        "    state = get_state()\n",
        "\n",
        "    test = i_episode % TARGET_UPDATE == 0\n",
        "\n",
        "    if test:\n",
        "        # target_netを更新。全ての重みやバイアスをコピー\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    for t in count():\n",
        "        # アクションを選択\n",
        "        action = select_action(state, test)\n",
        "        reward, done = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        next_state = get_state() if not done else None\n",
        "\n",
        "        # memoryにTrasitionを記録\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # モデル最適\n",
        "        optimize_model()\n",
        "\n",
        "        if done:\n",
        "            acc_sd, dis_sd = env.sd\n",
        "            acc_max, dis_max = env.max\n",
        "            print('{0:3}'.format(str(i_episode)), 'h_ave=', '{0:4.3f}'.format(env.h_ave), 'h_sd=', '{0:4.3f}'.format(env.h_sd), 'acc_sd=', '{0:4.3f}'.format(acc_sd), 'dis_sd=', '{0:4.3f}'.format(dis_sd), 'acc_max=', '{0:4.3f}'.format(acc_max), 'dis_max=', '{0:4.3f}'.format(dis_max), 'test=', '{0:5}'.format(str(test)))\n",
        "            break\n",
        "\n",
        "print('Complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCc80h9ocgt8"
      },
      "source": [
        "## 結果\n",
        "結果、目的である絶対加速度応答は一定程度低減されているようですが、低減効果は小さく、まだ改善の余地がありそうです。今回は、単純な制御法として減衰を変える方法にしてみましたが、報酬の与え方などを含めて色々と制御法はありそうです。より良い方法などあれば教えてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSmlSBuX4Git"
      },
      "source": [
        "## 参考\n",
        "以上のコードは、Githubで公開しています。\n",
        "\n",
        "[https://github.com/kakemotokeita/dqn-seismic-control](https://github.com/kakemotokeita/dqn-seismic-control)\n",
        "\n",
        "誤った記述や改善点などありましたら、issuesやPRなどで教えてください。"
      ]
    }
  ]
}